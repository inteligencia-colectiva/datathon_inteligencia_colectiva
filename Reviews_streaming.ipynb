{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.eventhub import EventHubConsumerClient\n",
    "from google.cloud import bigquery\n",
    "import json\n",
    "from google.api_core.exceptions import NotFound\n",
    "\n",
    "# Event Hub connection string and related configuration\n",
    "EVENTHUB_CONN_STR = 'Endpoint=sb://factored-datathon.servicebus.windows.net/;SharedAccessKeyName=datathon_group_5;SharedAccessKey=bhbWAqLTLi5/DASs+HOCYuwO0Kf5QRS4x+AEhMZauUE=;EntityPath=factored_datathon_amazon_reviews_5'\n",
    "CONSUMER_GROUP = 'inteligencia_colectiva' \n",
    "EVENTHUB_NAME = 'factored_datathon_amazon_reviews_5'\n",
    "\n",
    "# Configuration variables for Google BigQuery\n",
    "review_keys = [\"asin\", \"overall\", \"reviewText\", \"reviewerID\", \"reviewerName\", \"style\", \"summary\", \"unixReviewTime\", \"verified\", \"vote\"]\n",
    "bigquery_client = bigquery.Client()\n",
    "project_id = \"datathon-intel-colectiva\"\n",
    "dataset_id = \"ml_data\"\n",
    "table = \"reviews\"\n",
    "table_id = \"{}.{}.{}\".format(project_id, dataset_id, table)\n",
    "\n",
    "streaming_data = []\n",
    "batch_size = 3000  # Number of events to save in one batch\n",
    "event_count = 0\n",
    "batch_count = 0\n",
    "total = 0\n",
    "\n",
    "def normalize_json(parsed_json):\n",
    "    norm_json = {key: parsed_json[key] if key in parsed_json else \"\" for key in review_keys}\n",
    "    if norm_json['vote'] == \"\":\n",
    "        norm_json['vote'] = 1\n",
    "    return norm_json\n",
    "\n",
    "def load_bq(parsed_json_list):\n",
    "    try:\n",
    "        table = bigquery_client.get_table(table_id)\n",
    "    except NotFound:\n",
    "        # If the table does not exist, create it\n",
    "        schema = [\n",
    "            bigquery.SchemaField(name, 'STRING') for name in review_keys\n",
    "        ]\n",
    "        table = bigquery.Table(table_id, schema=schema)\n",
    "        table = bigquery_client.create_table(table)\n",
    "\n",
    "    # Insert data into the table\n",
    "    errors = bigquery_client.insert_rows_json(table, parsed_json_list)\n",
    "\n",
    "    if not errors:\n",
    "        return f\"Data uploaded successfully to {dataset_id}.{table_id}\"\n",
    "    else:\n",
    "        print(errors)\n",
    "        return \"Errors occurred while uploading data to BigQuery\"\n",
    "\n",
    "def upload_batch():\n",
    "    global streaming_data, event_count, batch_count, total\n",
    "\n",
    "    if streaming_data:\n",
    "        load_bq(streaming_data)\n",
    "        print('Batch uploaded')\n",
    "        total += event_count\n",
    "        event_count = 0\n",
    "        batch_count += 1\n",
    "        streaming_data = []\n",
    "\n",
    "def on_event(partition_context, event):\n",
    "    global event_count, streaming_data\n",
    "\n",
    "    try:\n",
    "        parsed_json = json.loads(event.body_as_str())\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Failed to parse JSON data from the event body.\")\n",
    "        return\n",
    "\n",
    "    row = normalize_json(parsed_json)\n",
    "    print(row)\n",
    "    streaming_data.append(row)\n",
    "    event_count += 1\n",
    "\n",
    "    if event_count >= batch_size:\n",
    "        upload_batch()\n",
    "\n",
    "# Create the EventHubConsumerClient\n",
    "client = EventHubConsumerClient.from_connection_string(\n",
    "    EVENTHUB_CONN_STR, consumer_group=CONSUMER_GROUP, eventhub_name=EVENTHUB_NAME\n",
    ")\n",
    "\n",
    "try:\n",
    "    with client:\n",
    "        # This will start the consumer to receive events\n",
    "        client.receive(on_event=on_event, starting_position=\"-1\")\n",
    "except KeyboardInterrupt:\n",
    "    # Upload any remaining events before exiting\n",
    "    upload_batch()\n",
    "    print(\"Receiving has stopped.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
